1. What is the difference between Precision and Recall?
Precision and Recall are evaluation metrics used in binary classification, especially when dealing with imbalanced datasets.

ðŸ”¹ Precision (Positive Predictive Value)
Definition: Out of all instances predicted positive, how many are actually positive.
â€‹
 
Interpretation: High precision means fewer false positives.

ðŸ”¹ Recall (Sensitivity or True Positive Rate)
Definition: Out of all actual positive instances, how many were correctly predicted.

â€‹
 
Interpretation: High recall means fewer false negatives.

âœ… Use Case Examples:
Precision-focused: Email spam detection (you don't want to wrongly classify important emails as spam).

Recall-focused: Disease screening (you want to catch all possible cases, even if some false alarms happen).

2. What is cross-validation, and why is it important in binary classification?
ðŸ”¹ What is Cross-Validation?
Cross-validation is a resampling technique used to assess the generalization ability of a model. It involves splitting the dataset into multiple training and validation sets, ensuring the model is tested on unseen data.

ðŸ”¹ How it works (e.g., K-Fold Cross-Validation):
The dataset is split into K equal parts (folds).

The model is trained on K-1 folds and validated on the remaining fold.

This process is repeated K times, each time with a different validation fold.

The average performance across folds gives a robust estimate.

âœ… Why important in binary classification?
Reduces the risk of overfitting or underfitting.

Provides a more reliable estimate of model performance than a single train-test split.

Helps in hyperparameter tuning.

Especially useful when the dataset is imbalanced or small, as it maximizes data usage for training and validation.

